{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Notebook to ModelOp Center:\n",
    "## Training, Evaluating, and Conforming a Model for Deployment\n",
    "\n",
    "\n",
    "In this notebook, we demonstrate the process of \n",
    "1. training a model, \n",
    "2. evaluating its performace, \n",
    "3. saving it for later use,\n",
    "4. and conforming it to MOC standards.\n",
    "\n",
    "More specifically, we will train a logistic regression classifier on the German Credit Data dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I - Model Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by loading relevant libraries. We will need `sklearn` for model training, and `aequitas` for bias detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "\n",
    "from aequitas.bias import Bias\n",
    "from aequitas.group import Group\n",
    "from aequitas.preprocessing import preprocess_input_df\n",
    "\n",
    "from sklearn import set_config\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, \\\n",
    "                            f1_score, fbeta_score, confusion_matrix\n",
    "\n",
    "# set_config(display='diagram')\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **German Credit Data** dataset can be found here: https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data). Download it and load it from a *CSV* file. For our purposes, the dataset has been modified slightly to include an `id` column, and a `gender` column (engineered from `status_sex`, used to demonstarte bias). The target variable is under `label`. We have mapped the labels `[1,2]` to `[0,1]`, where `1` indicates the positive class (loan default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"german_credit_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['id', 'duration_months', 'credit_amount', 'installment_rate',\n",
       "       'present_residence_since', 'age_years', 'number_existing_credits',\n",
       "       'checking_status', 'credit_history', 'purpose', 'savings_account',\n",
       "       'present_employment_since', 'debtors_guarantors', 'property',\n",
       "       'installment_plans', 'housing', 'job', 'number_people_liable',\n",
       "       'telephone', 'foreign_worker', 'gender', 'label'], dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "data.columns.values"
   ]
  },
  {
   "source": [
    "The aequitas library requires the true label to be encoded as 'label_value', so let us rename that column."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.rename(columns={\"label\":\"label_value\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   id  duration_months  credit_amount  installment_rate  \\\n",
       "0   0                6           1169                 4   \n",
       "1   1               48           5951                 2   \n",
       "2   2               12           2096                 2   \n",
       "3   3               42           7882                 2   \n",
       "4   4               24           4870                 3   \n",
       "\n",
       "   present_residence_since  age_years  number_existing_credits  \\\n",
       "0                        4         67                        2   \n",
       "1                        2         22                        1   \n",
       "2                        3         49                        1   \n",
       "3                        4         45                        1   \n",
       "4                        4         53                        2   \n",
       "\n",
       "  checking_status credit_history purpose  ... debtors_guarantors property  \\\n",
       "0             A11            A34     A43  ...               A101     A121   \n",
       "1             A12            A32     A43  ...               A101     A121   \n",
       "2             A14            A34     A46  ...               A101     A121   \n",
       "3             A11            A32     A42  ...               A103     A122   \n",
       "4             A11            A33     A40  ...               A101     A124   \n",
       "\n",
       "  installment_plans housing   job number_people_liable telephone  \\\n",
       "0              A143    A152  A173                    1      A192   \n",
       "1              A143    A152  A173                    1      A191   \n",
       "2              A143    A152  A172                    2      A191   \n",
       "3              A143    A153  A173                    2      A191   \n",
       "4              A143    A153  A173                    2      A191   \n",
       "\n",
       "   foreign_worker  gender label_value  \n",
       "0            A201    male           0  \n",
       "1            A201  female           1  \n",
       "2            A201    male           0  \n",
       "3            A201    male           0  \n",
       "4            A201    male           1  \n",
       "\n",
       "[5 rows x 22 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>duration_months</th>\n      <th>credit_amount</th>\n      <th>installment_rate</th>\n      <th>present_residence_since</th>\n      <th>age_years</th>\n      <th>number_existing_credits</th>\n      <th>checking_status</th>\n      <th>credit_history</th>\n      <th>purpose</th>\n      <th>...</th>\n      <th>debtors_guarantors</th>\n      <th>property</th>\n      <th>installment_plans</th>\n      <th>housing</th>\n      <th>job</th>\n      <th>number_people_liable</th>\n      <th>telephone</th>\n      <th>foreign_worker</th>\n      <th>gender</th>\n      <th>label_value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>6</td>\n      <td>1169</td>\n      <td>4</td>\n      <td>4</td>\n      <td>67</td>\n      <td>2</td>\n      <td>A11</td>\n      <td>A34</td>\n      <td>A43</td>\n      <td>...</td>\n      <td>A101</td>\n      <td>A121</td>\n      <td>A143</td>\n      <td>A152</td>\n      <td>A173</td>\n      <td>1</td>\n      <td>A192</td>\n      <td>A201</td>\n      <td>male</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>48</td>\n      <td>5951</td>\n      <td>2</td>\n      <td>2</td>\n      <td>22</td>\n      <td>1</td>\n      <td>A12</td>\n      <td>A32</td>\n      <td>A43</td>\n      <td>...</td>\n      <td>A101</td>\n      <td>A121</td>\n      <td>A143</td>\n      <td>A152</td>\n      <td>A173</td>\n      <td>1</td>\n      <td>A191</td>\n      <td>A201</td>\n      <td>female</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>12</td>\n      <td>2096</td>\n      <td>2</td>\n      <td>3</td>\n      <td>49</td>\n      <td>1</td>\n      <td>A14</td>\n      <td>A34</td>\n      <td>A46</td>\n      <td>...</td>\n      <td>A101</td>\n      <td>A121</td>\n      <td>A143</td>\n      <td>A152</td>\n      <td>A172</td>\n      <td>2</td>\n      <td>A191</td>\n      <td>A201</td>\n      <td>male</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>42</td>\n      <td>7882</td>\n      <td>2</td>\n      <td>4</td>\n      <td>45</td>\n      <td>1</td>\n      <td>A11</td>\n      <td>A32</td>\n      <td>A42</td>\n      <td>...</td>\n      <td>A103</td>\n      <td>A122</td>\n      <td>A143</td>\n      <td>A153</td>\n      <td>A173</td>\n      <td>2</td>\n      <td>A191</td>\n      <td>A201</td>\n      <td>male</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>24</td>\n      <td>4870</td>\n      <td>3</td>\n      <td>4</td>\n      <td>53</td>\n      <td>2</td>\n      <td>A11</td>\n      <td>A33</td>\n      <td>A40</td>\n      <td>...</td>\n      <td>A101</td>\n      <td>A124</td>\n      <td>A143</td>\n      <td>A153</td>\n      <td>A173</td>\n      <td>2</td>\n      <td>A191</td>\n      <td>A201</td>\n      <td>male</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 22 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all numeric columns need to be considered as numeric features. For example, `number_people_liable` only has two unique discrete values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1    845\n",
       "2    155\n",
       "Name: number_people_liable, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "data.number_people_liable.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may therefore treat it as a categorical feature. Note, however, that we may need to reconsider this option if more values appear in testing phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.number_people_liable = data.number_people_liable.astype('object')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding any further with model development, let us split the original dataset into two sets: a **baseline** set that will be used as a reference set, and a **sample** set which will mimic input data to the model once the model is in use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_baseline, df_sample = train_test_split(data, train_size=0.8, random_state=0)\n",
    "\n",
    "df_baseline.to_json('df_baseline.json', orient='records', lines=True)\n",
    "df_sample.to_json('df_sample.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data still contains non-predictive features, such as `id`, `label_value` and `gender` (excluded to remove explicit bias). We remove these below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictive_features = [\n",
    "    f for f in list(data.columns.values) \n",
    "    if f not in ['id', 'label_value', 'gender']\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, let us see which features are automatically encoded as **numeric**, and which are encoded as **categorical**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [\n",
    "    f for f in list(data.select_dtypes(include=['category', 'object'])) \n",
    "    if f in predictive_features\n",
    "]\n",
    "\n",
    "numeric_features = [\n",
    "    f for f in predictive_features \n",
    "    if f not in categorical_features\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Categorical features**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['checking_status', 'credit_history', 'purpose', 'savings_account', 'present_employment_since', 'debtors_guarantors', 'property', 'installment_plans', 'housing', 'job', 'number_people_liable', 'telephone', 'foreign_worker']\n"
     ]
    }
   ],
   "source": [
    "print(categorical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Numeric features**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['duration_months', 'credit_amount', 'installment_rate', 'present_residence_since', 'age_years', 'number_existing_credits']\n"
     ]
    }
   ],
   "source": [
    "print(numeric_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything looks good; let us proceed with training. We need to specify **predictive** and **response** variables for each of the training and test sets. We set these by filtering the baseline and sample sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_baseline[predictive_features]\n",
    "X_test = df_sample[predictive_features]\n",
    "\n",
    "y_train = df_baseline['label_value']\n",
    "y_test = df_sample['label_value']\n",
    "\n",
    "X_train.to_json('X_train.json', orient='records', lines=True)\n",
    "X_test.to_json('X_test.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train a **Logistic Regression** classifier. Since our data contains categorical features, we will need to encode the data first. We'll use `pd.get_dummies()` for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.get_dummies(X_train)\n",
    "X_test = pd.get_dummies(X_test)\n",
    "\n",
    "# saving the final list of columns from getting dummies on X_train\n",
    "predictive_features = list(X_train.columns)\n",
    "pickle.dump(predictive_features, open('predictive_features.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may now fit the classifier to the training data. Since \"it is worse to classify a customer as good when they are bad, than it is to classify a customer as bad when they are good\", we will use an **F_beta metric**, with `beta=2`, to judge the performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=50, class_weight='balanced', cv=5, dual=False,\n",
       "                     fit_intercept=True, intercept_scaling=1.0, l1_ratios=None,\n",
       "                     max_iter=1000, multi_class='auto', n_jobs=-1, penalty='l2',\n",
       "                     random_state=0, refit=True,\n",
       "                     scoring=make_scorer(fbeta_score, beta=2), solver='lbfgs',\n",
       "                     tol=0.0001, verbose=0)"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "# train logistic regression model\n",
    "logreg = LogisticRegressionCV(\n",
    "    Cs=50,\n",
    "    cv=5,\n",
    "    max_iter=1000,\n",
    "    n_jobs=-1,\n",
    "    scoring=make_scorer(fbeta_score, beta=2),\n",
    "    class_weight='balanced',\n",
    "    random_state=0\n",
    ")\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7274119448698314"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "logreg.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.6230529595015577"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "logreg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**II - Model Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before saving our trained model for further use, let's look at some performance metrics. We will evaluate the model on both the training and test sets; we would like to see a stable performance.\n",
    "\n",
    "For repeatability, let's define a function which computes multiple metrics at-a-time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y, y_preds):\n",
    "    \"\"\"\n",
    "    A function to evaluate a classification model\n",
    "    \n",
    "    param: y: true (actual) labels\n",
    "    param: y_preds: predicted labels (as scored by model)\n",
    "    \n",
    "    return: mutiple classification performance metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    return [\n",
    "        accuracy_score(y, y_preds),\n",
    "        precision_score(y, y_preds),\n",
    "        recall_score(y, y_preds),\n",
    "        f1_score(y, y_preds),\n",
    "        fbeta_score(y, y_preds, beta=2),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now compute predictions on both training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_preds = logreg.predict(X_test)\n",
    "y_train_preds = logreg.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will display performance metrics in a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "preformance_df = pd.DataFrame(\n",
    "    data=[{}],\n",
    "    columns=['Accuracy', 'Precision', 'Recall', 'F1 score', 'F2 Score'],\n",
    "    index=['Training Set', 'Test Set']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "preformance_df.loc['Training Set',:] = compute_metrics(y=y_train, y_preds=y_train_preds)\n",
    "preformance_df.loc['Test Set',:] = compute_metrics(y=y_test, y_preds=y_test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how our model performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "              Accuracy  Precision    Recall  F1 score  F2 Score\n",
       "Training Set     0.750   0.562130  0.785124  0.655172  0.727412\n",
       "Test Set         0.665   0.449438  0.689655  0.544218  0.623053"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1 score</th>\n      <th>F2 Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Training Set</th>\n      <td>0.750</td>\n      <td>0.562130</td>\n      <td>0.785124</td>\n      <td>0.655172</td>\n      <td>0.727412</td>\n    </tr>\n    <tr>\n      <th>Test Set</th>\n      <td>0.665</td>\n      <td>0.449438</td>\n      <td>0.689655</td>\n      <td>0.544218</td>\n      <td>0.623053</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "preformance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it's good to see that the performance on the training set is not too far off from the performance on the test set, further model improvements are needed to achieve better F2 scores. For now, we will contend with this model and use it to produce new predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**III - Saving and Loading the Trained Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is **trained** and **evaluated**, we save it in a binary format. It will then be loaded and used to make new predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(logreg, open(\"logreg_classifier.pickle\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is reloaded on-demand as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_classifier = pickle.load(open(\"logreg_classifier.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions are produced on-demand by calling the `predict()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_preds = logreg_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IV - Using a SHAP explainer model to explain feature importance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shap is used to explain feature importance, a functionality that is supported by ModelOp Center. We'll train an `explainer` and pickle it for use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'job_A171': 3.1328221263664673e-05,\n",
       " 'purpose_A410': 0.00020948758335020458,\n",
       " 'purpose_A44': 0.0003090591737098107,\n",
       " 'purpose_A48': 0.0008766581825881572,\n",
       " 'debtors_guarantors_A101': 0.0009150508136599006,\n",
       " 'housing_A153': 0.0016935646612710384,\n",
       " 'present_employment_since_A71': 0.0017089631277168033,\n",
       " 'savings_account_A63': 0.0020129763606446262,\n",
       " 'purpose_A45': 0.0020844768187666747,\n",
       " 'foreign_worker_A201': 0.003766964306474573,\n",
       " 'purpose_A42': 0.0039455273948171485,\n",
       " 'checking_status_A13': 0.006138224622855559,\n",
       " 'installment_plans_A142': 0.006372036544310882,\n",
       " 'job_A172': 0.006583504312811813,\n",
       " 'foreign_worker_A202': 0.006782362236824462,\n",
       " 'number_existing_credits': 0.00750011359963024,\n",
       " 'purpose_A49': 0.007500582081896385,\n",
       " 'savings_account_A64': 0.00801772942021382,\n",
       " 'credit_history_A33': 0.00805456018203451,\n",
       " 'debtors_guarantors_A102': 0.009021527006583886,\n",
       " 'credit_history_A30': 0.011292419424637157,\n",
       " 'savings_account_A62': 0.01194298101274311,\n",
       " 'property_A123': 0.012660923236592878,\n",
       " 'debtors_guarantors_A103': 0.01381481850812078,\n",
       " 'credit_history_A31': 0.014032983160095276,\n",
       " 'number_people_liable_2': 0.015317375151231539,\n",
       " 'job_A174': 0.01674088696555357,\n",
       " 'credit_history_A32': 0.017734229446707925,\n",
       " 'purpose_A46': 0.019873506687696822,\n",
       " 'property_A122': 0.01995330431488622,\n",
       " 'present_employment_since_A73': 0.02237516182532416,\n",
       " 'installment_plans_A141': 0.027711132342581327,\n",
       " 'number_people_liable_1': 0.029942891113355786,\n",
       " 'present_employment_since_A75': 0.03893752534980459,\n",
       " 'property_A124': 0.039322747899443185,\n",
       " 'housing_A151': 0.043992108858815444,\n",
       " 'present_residence_since': 0.04729293278716826,\n",
       " 'telephone_A191': 0.05240518751403526,\n",
       " 'purpose_A41': 0.05539897377916361,\n",
       " 'job_A173': 0.06528054604680217,\n",
       " 'present_employment_since_A74': 0.06728753336749936,\n",
       " 'checking_status_A12': 0.06828473759183341,\n",
       " 'present_employment_since_A72': 0.07087086711525754,\n",
       " 'installment_plans_A143': 0.07795073352914118,\n",
       " 'telephone_A192': 0.07971655475877716,\n",
       " 'housing_A152': 0.08128517023081643,\n",
       " 'savings_account_A65': 0.09052881652812012,\n",
       " 'purpose_A43': 0.09734556985402298,\n",
       " 'property_A121': 0.11331949356374771,\n",
       " 'purpose_A40': 0.11944105896776773,\n",
       " 'credit_amount': 0.13377257721960387,\n",
       " 'savings_account_A61': 0.13647436684151096,\n",
       " 'age_years': 0.148075573075796,\n",
       " 'credit_history_A34': 0.18972961710340905,\n",
       " 'checking_status_A11': 0.1940913502412228,\n",
       " 'installment_rate': 0.1942918160734251,\n",
       " 'duration_months': 0.294281718102304,\n",
       " 'checking_status_A14': 0.32966646260987026}"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "# training the explainer on the model and train data\n",
    "explainer = shap.LinearExplainer(logreg_classifier, X_train, feature_name=X_train.columns)\n",
    "\n",
    "# getting shap values for the test data\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# re-organizing and sorting the data\n",
    "shap_values = np.mean(abs(shap_values), axis=0).tolist()\n",
    "shap_values = dict(zip(X_train.columns, shap_values))\n",
    "sorted_shap_values = {k:v for k, v in sorted(shap_values.items(),\n",
    "                                             key=lambda x: x[1])}\n",
    "\n",
    "# show the values\n",
    "sorted_shap_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(explainer, open('explainer.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**V - Saving `_scored.json` datasets for further analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The monitoring libraries need `score` (predictions), `label_values` (ground truth), and `predicted_probs` (probability outputs from predictions) features to work correctly. To that end, let us produce the necessary prediction outputs and append them to our labeled baseline and sample sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_baseline_scored = df_baseline.copy(deep=True)\n",
    "df_sample_scored = df_sample.copy(deep=True)\n",
    "\n",
    "# recording scores (predictions)\n",
    "df_baseline_scored[\"score\"] = logreg_classifier.predict(\n",
    "    pd.get_dummies(df_baseline)[predictive_features])\n",
    "# recording predicted probabilities\n",
    "df_baseline_scored['predicted_probs'] = [y for x, y in logreg_classifier.predict_proba(\n",
    "    pd.get_dummies(df_baseline)[predictive_features])]\n",
    "\n",
    "# recording scores (predictions)\n",
    "df_sample_scored[\"score\"] = logreg_classifier.predict(\n",
    "    pd.get_dummies(df_sample)[predictive_features])\n",
    "df_sample_scored['predicted_probs'] = [y for x, y in logreg_classifier.predict_proba(\n",
    "    pd.get_dummies(df_sample)[predictive_features])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save these two DataFrames before proceeding further:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_baseline_scored.to_json('df_baseline_scored.json', orient='records', lines=True)\n",
    "df_sample_scored.to_json('df_sample_scored.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IV - Evaluating Bias on Protected Classes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `gender` is a protected class, we have excluded from the list of predictive features. However, this does not guarantee that the model is not implicitely biased, as `gender` could potentially be inferred from other features. It is therefore imperative that we evaluate our model for Bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we call the aequitas preprocessing function on our datasets, filtered to the features we care about: `score` (prediction), `label_value` (true label), and `gender` (protected class):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_baseline_scored_processed, _ = preprocess_input_df(\n",
    "    df_baseline_scored.loc[:,['score', 'label_value', 'gender']]\n",
    ")\n",
    "df_sample_scored_processed, _ = preprocess_input_df(\n",
    "    df_sample_scored.loc[:,['score', 'label_value', 'gender']]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by computing some `Group` Metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_baseline, g_sample = Group(), Group()\n",
    "xtab_baseline, _ = g_baseline.get_crosstabs(df_baseline_scored_processed)\n",
    "xtab_sample, _ = g_sample.get_crosstabs(df_sample_scored_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "absolute_metrics_baseline = g_baseline.list_absolute_metrics(xtab_baseline)\n",
    "absolute_metrics_sample = g_sample.list_absolute_metrics(xtab_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the absolute metrics, computed on baseline and sample sets, respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  attribute_name attribute_value   tpr   tnr   for   fdr   fpr   fnr   npv  \\\n",
       "0         gender          female  0.83  0.73  0.11  0.38  0.27  0.17  0.89   \n",
       "1         gender            male  0.76  0.74  0.11  0.47  0.26  0.24  0.89   \n",
       "\n",
       "   precision   ppr  pprev  prev  \n",
       "0       0.62  0.33   0.47  0.35  \n",
       "1       0.53  0.67   0.40  0.28  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>attribute_name</th>\n      <th>attribute_value</th>\n      <th>tpr</th>\n      <th>tnr</th>\n      <th>for</th>\n      <th>fdr</th>\n      <th>fpr</th>\n      <th>fnr</th>\n      <th>npv</th>\n      <th>precision</th>\n      <th>ppr</th>\n      <th>pprev</th>\n      <th>prev</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>gender</td>\n      <td>female</td>\n      <td>0.83</td>\n      <td>0.73</td>\n      <td>0.11</td>\n      <td>0.38</td>\n      <td>0.27</td>\n      <td>0.17</td>\n      <td>0.89</td>\n      <td>0.62</td>\n      <td>0.33</td>\n      <td>0.47</td>\n      <td>0.35</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>gender</td>\n      <td>male</td>\n      <td>0.76</td>\n      <td>0.74</td>\n      <td>0.11</td>\n      <td>0.47</td>\n      <td>0.26</td>\n      <td>0.24</td>\n      <td>0.89</td>\n      <td>0.53</td>\n      <td>0.67</td>\n      <td>0.40</td>\n      <td>0.28</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "xtab_baseline[['attribute_name', 'attribute_value'] + absolute_metrics_baseline].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  attribute_name attribute_value   tpr   tnr   for   fdr   fpr   fnr   npv  \\\n",
       "0         gender          female  0.64  0.74  0.20  0.43  0.26  0.36  0.80   \n",
       "1         gender            male  0.73  0.61  0.13  0.61  0.39  0.27  0.87   \n",
       "\n",
       "   precision   ppr  pprev  prev  \n",
       "0       0.57  0.31   0.39  0.35  \n",
       "1       0.39  0.69   0.48  0.26  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>attribute_name</th>\n      <th>attribute_value</th>\n      <th>tpr</th>\n      <th>tnr</th>\n      <th>for</th>\n      <th>fdr</th>\n      <th>fpr</th>\n      <th>fnr</th>\n      <th>npv</th>\n      <th>precision</th>\n      <th>ppr</th>\n      <th>pprev</th>\n      <th>prev</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>gender</td>\n      <td>female</td>\n      <td>0.64</td>\n      <td>0.74</td>\n      <td>0.20</td>\n      <td>0.43</td>\n      <td>0.26</td>\n      <td>0.36</td>\n      <td>0.80</td>\n      <td>0.57</td>\n      <td>0.31</td>\n      <td>0.39</td>\n      <td>0.35</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>gender</td>\n      <td>male</td>\n      <td>0.73</td>\n      <td>0.61</td>\n      <td>0.13</td>\n      <td>0.61</td>\n      <td>0.39</td>\n      <td>0.27</td>\n      <td>0.87</td>\n      <td>0.39</td>\n      <td>0.69</td>\n      <td>0.48</td>\n      <td>0.26</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "xtab_sample[['attribute_name', 'attribute_value'] + absolute_metrics_sample].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also add some raw counts (group sizes) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   model_id score_threshold    k attribute_name attribute_value   pp   pn  \\\n",
       "0         0      binary 0/1  338         gender          female  112  126   \n",
       "1         0      binary 0/1  338         gender            male  226  336   \n",
       "\n",
       "    fp  fn   tn   tp  group_label_pos  group_label_neg  group_size  \\\n",
       "0   42  14  112   70               84              154         238   \n",
       "1  106  38  298  120              158              404         562   \n",
       "\n",
       "   total_entities  \n",
       "0             800  \n",
       "1             800  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model_id</th>\n      <th>score_threshold</th>\n      <th>k</th>\n      <th>attribute_name</th>\n      <th>attribute_value</th>\n      <th>pp</th>\n      <th>pn</th>\n      <th>fp</th>\n      <th>fn</th>\n      <th>tn</th>\n      <th>tp</th>\n      <th>group_label_pos</th>\n      <th>group_label_neg</th>\n      <th>group_size</th>\n      <th>total_entities</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>binary 0/1</td>\n      <td>338</td>\n      <td>gender</td>\n      <td>female</td>\n      <td>112</td>\n      <td>126</td>\n      <td>42</td>\n      <td>14</td>\n      <td>112</td>\n      <td>70</td>\n      <td>84</td>\n      <td>154</td>\n      <td>238</td>\n      <td>800</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>binary 0/1</td>\n      <td>338</td>\n      <td>gender</td>\n      <td>male</td>\n      <td>226</td>\n      <td>336</td>\n      <td>106</td>\n      <td>38</td>\n      <td>298</td>\n      <td>120</td>\n      <td>158</td>\n      <td>404</td>\n      <td>562</td>\n      <td>800</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "xtab_baseline[[col for col in xtab_baseline.columns if col not in absolute_metrics_baseline]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   model_id score_threshold   k attribute_name attribute_value  pp  pn  fp  \\\n",
       "0         0      binary 0/1  89         gender          female  28  44  12   \n",
       "1         0      binary 0/1  89         gender            male  61  67  37   \n",
       "\n",
       "   fn  tn  tp  group_label_pos  group_label_neg  group_size  total_entities  \n",
       "0   9  35  16               25               47          72             200  \n",
       "1   9  58  24               33               95         128             200  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model_id</th>\n      <th>score_threshold</th>\n      <th>k</th>\n      <th>attribute_name</th>\n      <th>attribute_value</th>\n      <th>pp</th>\n      <th>pn</th>\n      <th>fp</th>\n      <th>fn</th>\n      <th>tn</th>\n      <th>tp</th>\n      <th>group_label_pos</th>\n      <th>group_label_neg</th>\n      <th>group_size</th>\n      <th>total_entities</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>binary 0/1</td>\n      <td>89</td>\n      <td>gender</td>\n      <td>female</td>\n      <td>28</td>\n      <td>44</td>\n      <td>12</td>\n      <td>9</td>\n      <td>35</td>\n      <td>16</td>\n      <td>25</td>\n      <td>47</td>\n      <td>72</td>\n      <td>200</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>binary 0/1</td>\n      <td>89</td>\n      <td>gender</td>\n      <td>male</td>\n      <td>61</td>\n      <td>67</td>\n      <td>37</td>\n      <td>9</td>\n      <td>58</td>\n      <td>24</td>\n      <td>33</td>\n      <td>95</td>\n      <td>128</td>\n      <td>200</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "xtab_sample[[col for col in xtab_sample.columns if col not in absolute_metrics_sample]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it for `Group` metrics. Let's move on to `Bias` metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "get_disparity_predefined_group()\nget_disparity_predefined_group()\n"
     ]
    }
   ],
   "source": [
    "b_baseline, b_sample = Bias(), Bias()\n",
    "\n",
    "bdf_baseline = b_baseline.get_disparity_predefined_groups(\n",
    "    xtab_baseline, \n",
    "    original_df=df_baseline_scored_processed, \n",
    "    ref_groups_dict={'gender':'male'}, alpha=0.05, mask_significance=True\n",
    ")\n",
    "\n",
    "bdf_sample = b_sample.get_disparity_predefined_groups(\n",
    "    xtab_sample, \n",
    "    original_df=df_sample_scored_processed, \n",
    "    ref_groups_dict={'gender':'male'}, alpha=0.05, mask_significance=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compute **disparity** metrics as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculated_disparities_baseline = b_baseline.list_disparities(bdf_baseline)\n",
    "calculated_disparities_sample = b_sample.list_disparities(bdf_sample)\n",
    "\n",
    "disparity_metrics_df_baseline = bdf_baseline[\n",
    "    ['attribute_name', 'attribute_value'] + \\\n",
    "        calculated_disparities_baseline\n",
    "    ]\n",
    "disparity_metrics_df_sample = bdf_sample[\n",
    "    ['attribute_name', 'attribute_value'] + \\\n",
    "        calculated_disparities_sample\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the computed disparity metrics on baseline and sample sets, respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  attribute_name attribute_value  ppr_disparity  pprev_disparity  \\\n",
       "0         gender          female       0.495575         1.170224   \n",
       "1         gender            male       1.000000         1.000000   \n",
       "\n",
       "   precision_disparity  fdr_disparity  for_disparity  fpr_disparity  \\\n",
       "0             1.177083       0.799528       0.982456       1.039451   \n",
       "1             1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "   fnr_disparity  tpr_disparity  tnr_disparity  npv_disparity  \n",
       "0       0.692982       1.097222       0.985967       1.002237  \n",
       "1       1.000000       1.000000       1.000000       1.000000  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>attribute_name</th>\n      <th>attribute_value</th>\n      <th>ppr_disparity</th>\n      <th>pprev_disparity</th>\n      <th>precision_disparity</th>\n      <th>fdr_disparity</th>\n      <th>for_disparity</th>\n      <th>fpr_disparity</th>\n      <th>fnr_disparity</th>\n      <th>tpr_disparity</th>\n      <th>tnr_disparity</th>\n      <th>npv_disparity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>gender</td>\n      <td>female</td>\n      <td>0.495575</td>\n      <td>1.170224</td>\n      <td>1.177083</td>\n      <td>0.799528</td>\n      <td>0.982456</td>\n      <td>1.039451</td>\n      <td>0.692982</td>\n      <td>1.097222</td>\n      <td>0.985967</td>\n      <td>1.002237</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>gender</td>\n      <td>male</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "disparity_metrics_df_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  attribute_name attribute_value  ppr_disparity  pprev_disparity  \\\n",
       "0         gender          female       0.459016         0.816029   \n",
       "1         gender            male       1.000000         1.000000   \n",
       "\n",
       "   precision_disparity  fdr_disparity  for_disparity  fpr_disparity  \\\n",
       "0             1.452381       0.706564       1.522727       0.655549   \n",
       "1             1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "   fnr_disparity  tpr_disparity  tnr_disparity  npv_disparity  \n",
       "0           1.32           0.88       1.219736       0.918887  \n",
       "1           1.00           1.00       1.000000       1.000000  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>attribute_name</th>\n      <th>attribute_value</th>\n      <th>ppr_disparity</th>\n      <th>pprev_disparity</th>\n      <th>precision_disparity</th>\n      <th>fdr_disparity</th>\n      <th>for_disparity</th>\n      <th>fpr_disparity</th>\n      <th>fnr_disparity</th>\n      <th>tpr_disparity</th>\n      <th>tnr_disparity</th>\n      <th>npv_disparity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>gender</td>\n      <td>female</td>\n      <td>0.459016</td>\n      <td>0.816029</td>\n      <td>1.452381</td>\n      <td>0.706564</td>\n      <td>1.522727</td>\n      <td>0.655549</td>\n      <td>1.32</td>\n      <td>0.88</td>\n      <td>1.219736</td>\n      <td>0.918887</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>gender</td>\n      <td>male</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.00</td>\n      <td>1.00</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "disparity_metrics_df_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the disparity metrics above are worrisome! We might need to retrain the model, possibly with better feature engineering. That's an exercise for a later time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**V - Conforming Model Code to MOC Requirements**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conformace is best-demonstrated through and example. Let's look at the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import shap\n",
    "from aequitas.preprocessing import preprocess_input_df\n",
    "from aequitas.group import Group\n",
    "from aequitas.bias import Bias\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, f1_score, confusion_matrix\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from scipy.stats import epps_singleton_2samp, gaussian_kde, ks_2samp\n",
    "\n",
    "\n",
    "# modelop.init\n",
    "def begin():\n",
    "\n",
    "    global logreg_classifier\n",
    "    global predictive_features\n",
    "    global explainer\n",
    "\n",
    "    # load pickled logistic regression model\n",
    "    logreg_classifier = pickle.load(open(\"logreg_classifier.pickle\", \"rb\"))\n",
    "\n",
    "    # load pickled predictive feature list\n",
    "    predictive_features = pickle.load(open(\"predictive_features.pickle\", \"rb\"))\n",
    "\n",
    "    # load shap explainer\n",
    "    explainer = pickle.load(open(\"explainer.pickle\", \"rb\"))\n",
    "\n",
    "\n",
    "def preprocess(data):\n",
    "    data_processed = data.copy(deep=True)\n",
    "    # There are only two unique values in data.number_people_liable.\n",
    "    # Treat it as a categorical feature\n",
    "    data_processed[\"number_people_liable\"] = data_processed[\"number_people_liable\"].astype(\"object\")\n",
    "\n",
    "    # one-hot encode data with pd.get_dummies()\n",
    "    data_processed = pd.get_dummies(data_processed)\n",
    "\n",
    "    # in case features don't exist that are needed for the model (possible when dummying)\n",
    "    # will create column of zeros for that feature\n",
    "    for col in predictive_features:\n",
    "        if col not in data_processed.columns:\n",
    "            data_processed[col] = np.zeros(data_processed.shape[0])\n",
    "\n",
    "    return data_processed\n",
    "\n",
    "\n",
    "# modelop.score\n",
    "def action(data):\n",
    "\n",
    "    # Turn data into DataFrame\n",
    "    data = pd.DataFrame(data)\n",
    "\n",
    "    # preprocess data\n",
    "    data_processed = preprocess(data)\n",
    "\n",
    "    # generate predictions\n",
    "    data[\"predicted_probs\"] = [\n",
    "        x[1] for x in logreg_classifier.predict_proba(data_processed[predictive_features])\n",
    "    ]\n",
    "    data[\"score\"] = logreg_classifier.predict(data_processed[predictive_features])\n",
    "\n",
    "    data = data[\n",
    "        [\n",
    "            'id',\n",
    "            'duration_months',\n",
    "            'credit_amount',\n",
    "            'installment_rate',\n",
    "            'present_residence_since',\n",
    "            'age_years',\n",
    "            'number_existing_credits',\n",
    "            'checking_status',\n",
    "            'credit_history',\n",
    "            'purpose',\n",
    "            'savings_account',\n",
    "            'present_employment_since',\n",
    "            'debtors_guarantors',\n",
    "            'property',\n",
    "            'installment_plans',\n",
    "            'housing',\n",
    "            'job',\n",
    "            'number_people_liable',\n",
    "            'telephone',\n",
    "            'foreign_worker',\n",
    "            'gender',\n",
    "            'label_value',\n",
    "            'score',\n",
    "            'predicted_probs'\n",
    "        ]\n",
    "    ]\n",
    "    # MOC expects the action function to be a *yield* function\n",
    "    yield data.to_dict(orient=\"records\")\n",
    "\n",
    "\n",
    "# modelop.metrics\n",
    "def metrics(df_baseline, data):\n",
    "    # dictionary to hold final metrics\n",
    "    metrics = {}\n",
    "\n",
    "    # convert data into DataFrame\n",
    "    data = pd.DataFrame(data)\n",
    "\n",
    "    # getting dummies for shap values\n",
    "    data_processed = preprocess(data)[predictive_features]\n",
    "\n",
    "    # calculate metrics\n",
    "    f1 = f1_score(data[\"label_value\"], data[\"score\"])\n",
    "    cm = confusion_matrix(data[\"label_value\"], data[\"score\"])\n",
    "    labels = [\"Default\", \"Pay Off\"]\n",
    "    cm = matrix_to_dicts(cm, labels)\n",
    "    fpr, tpr, thres = roc_curve(data[\"label_value\"], data[\"predicted_probs\"])\n",
    "    auc_val = roc_auc_score(data[\"label_value\"], data[\"predicted_probs\"])\n",
    "    roc = [{\"fpr\": x[0], \"tpr\": x[1]} for x in list(zip(fpr, tpr))]\n",
    "    \n",
    "    # assigning metrics to output dictionary\n",
    "    metrics[\"performance\"] = [\n",
    "        {\n",
    "            \"test_name\": \"Classification Metrics\",\n",
    "            \"test_category\": \"performance\",\n",
    "            \"test_type\": \"classification_metrics\",\n",
    "            \"test_id\": \"performance_classification_metrics\",\n",
    "            \"values\": {\n",
    "                \"f1_score\": f1,\n",
    "                \"auc\": auc_val,\n",
    "                \"confusion_matrix\": cm\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # top-level metrics\n",
    "    metrics[\"confusion_matrix\"] = cm\n",
    "    metrics[\"roc\"] = roc\n",
    "\n",
    "    # categorical/numerical columns for drift\n",
    "    categorical_features = [\n",
    "        f\n",
    "        for f in list(data.select_dtypes(include=[\"category\", \"object\"]))\n",
    "        if f in df_baseline.columns\n",
    "    ]\n",
    "    numerical_features = [\n",
    "        f for f in df_baseline.columns if f not in categorical_features\n",
    "    ]\n",
    "    numerical_features = [\n",
    "        x\n",
    "        for x in numerical_features\n",
    "        if x not in [\"id\", \"score\", \"label_value\", \"predicted_probs\"]\n",
    "    ]\n",
    "\n",
    "   \n",
    "    # assigning metrics to output dictionary\n",
    "    metrics[\"bias\"] = get_bias_metrics(data)\n",
    "    metrics[\"data_drift\"] = get_data_drift_metrics(\n",
    "        df_baseline, data, numerical_features, categorical_features\n",
    "    )\n",
    "    metrics[\"concept_drift\"] = get_concept_drift_metrics(\n",
    "        df_baseline, data\n",
    "    )\n",
    "    metrics[\"explainability\"] = [get_shap_values(data_processed)]\n",
    "\n",
    "    # MOC expects the action function to be a *yield* function\n",
    "    yield metrics\n",
    "\n",
    "\n",
    "def get_bias_metrics(data):\n",
    "    # To measure Bias towards gender, filter DataFrame\n",
    "    # to \"score\", \"label_value\" (ground truth), and\n",
    "    # \"gender\" (protected attribute)\n",
    "    data_scored = data[[\"score\", \"label_value\", \"gender\"]]\n",
    "\n",
    "    # Process DataFrame\n",
    "    data_scored_processed, _ = preprocess_input_df(data_scored)\n",
    "\n",
    "    # Group Metrics\n",
    "    g = Group()\n",
    "    xtab, _ = g.get_crosstabs(data_scored_processed)\n",
    "\n",
    "    # Absolute metrics, such as 'tpr', 'tnr','precision', etc.\n",
    "    absolute_metrics = g.list_absolute_metrics(xtab)\n",
    "\n",
    "    # DataFrame of calculated absolute metrics for each sample population group\n",
    "    absolute_metrics_df = xtab[\n",
    "        [\"attribute_name\", \"attribute_value\"] + absolute_metrics\n",
    "    ].round(2)\n",
    "\n",
    "    # Bias Metrics\n",
    "    b = Bias()\n",
    "\n",
    "    # Disparities calculated in relation gender for \"male\" and \"female\"\n",
    "    bias_df = b.get_disparity_predefined_groups(\n",
    "        xtab,\n",
    "        original_df=data_scored_processed,\n",
    "        ref_groups_dict={\"gender\": \"male\"},\n",
    "        alpha=0.05,\n",
    "        mask_significance=True,\n",
    "    )\n",
    "\n",
    "    # Disparity metrics added to bias DataFrame\n",
    "    calculated_disparities = b.list_disparities(bias_df)\n",
    "\n",
    "    disparity_metrics_df = bias_df[\n",
    "        [\"attribute_name\", \"attribute_value\"] + calculated_disparities\n",
    "    ]\n",
    "\n",
    "    # Output a JSON object of calculated metrics\n",
    "    return {\n",
    "        \"test_name\": \"Aequitas Bias\",\n",
    "        \"test_category\": \"bias\",\n",
    "        \"test_type\": \"bias\",\n",
    "        \"protected_class\": \"race\",\n",
    "        \"test_id\": \"bias_bias_gender\",\n",
    "        \"reference_group\": \"male\",\n",
    "        \"thresholds\": {\n",
    "            \"min\": 0.8,\n",
    "            \"max\": 1.25\n",
    "        },\n",
    "        \"values\": [\n",
    "            disparity_metrics_df.to_dict(orient=\"records\")\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def matrix_to_dicts(matrix, labels):\n",
    "    cm = []\n",
    "    for idx, label in enumerate(labels):\n",
    "        cm.append(dict(zip(labels, matrix[idx, :].tolist())))\n",
    "    return cm\n",
    "\n",
    "\n",
    "def get_data_drift_metrics(df_baseline, df_sample, numerical_cols, categorical_cols):\n",
    "    data_drift_metrics = []\n",
    "    data_drift_metrics.append(es_metric(df_baseline, df_sample, numerical_cols))\n",
    "    data_drift_metrics.append(ks_metric(df_baseline, df_sample, numerical_cols))\n",
    "    data_drift_metrics.append(\n",
    "        js_metric(\n",
    "            df_baseline, df_sample, numerical_cols, categorical_cols\n",
    "        )\n",
    "    )\n",
    "    return data_drift_metrics\n",
    "\n",
    "def get_concept_drift_metrics(df_baseline, df_sample):\n",
    "    concept_drift_metrics = []\n",
    "    concept_drift_metrics.append(es_metric(df_baseline, df_sample, [\"score\"]))\n",
    "    concept_drift_metrics.append(ks_metric(df_baseline, df_sample, [\"score\"]))\n",
    "    concept_drift_metrics.append(\n",
    "        js_metric(\n",
    "            df_baseline, df_sample, [\"score\"], []\n",
    "        )\n",
    "    )\n",
    "    return concept_drift_metrics\n",
    "\n",
    "\n",
    "def ks_metric(df1, df2, numerical_columns):\n",
    "    ks_tests = [\n",
    "        ks_2samp(data1=df1.loc[:, col], data2=df2.loc[:, col])\n",
    "        for col in numerical_columns\n",
    "    ]\n",
    "    p_values = [x[1] for x in ks_tests]\n",
    "    ks_pvalues = dict(zip(numerical_columns, p_values))\n",
    "    return {\n",
    "        \"test_name\": \"Kolmogorov-Smirnov\",\n",
    "        \"test_category\": \"data_drift\",\n",
    "        \"test_type\": \"kolmogorov_smirnov\",\n",
    "        \"metric\": \"p_value\",\n",
    "        \"test_id\": \"data_drift_kolmogorov_smirnov_p_value\",\n",
    "        \"values\": ks_pvalues\n",
    "    }\n",
    "\n",
    "\n",
    "def es_metric(df1, df2, numerical_columns):\n",
    "    es_tests = []\n",
    "    for col in numerical_columns:\n",
    "        try:\n",
    "            es_test = epps_singleton_2samp(x=df1.loc[:, col], y=df2.loc[:, col])\n",
    "        except np.linalg.LinAlgError:\n",
    "            es_test = [None, None]\n",
    "        es_tests.append(es_test)\n",
    "    p_values = [x[1] for x in es_tests]\n",
    "    es_pvalues = dict(zip(numerical_columns, p_values))\n",
    "    return {\n",
    "        \"test_name\": \"Epps-Singleton\",\n",
    "        \"test_category\": \"data_drift\",\n",
    "        \"test_type\": \"epps_singleton\",\n",
    "        \"metric\": \"p_value\",\n",
    "        \"test_id\": \"data_drift_epps_singleton_p_value\",\n",
    "        \"values\": es_pvalues\n",
    "    }\n",
    "\n",
    "\n",
    "def js_metric(df1, df2, numerical_columns, categorical_columns):\n",
    "    res = {}\n",
    "    STEPS = 100\n",
    "    for col in categorical_columns:\n",
    "        col_baseline = df1[col].to_frame()\n",
    "        col_sample = df2[col].to_frame()\n",
    "        col_baseline[\"source\"] = \"baseline\"\n",
    "        col_sample[\"source\"] = \"sample\"\n",
    "\n",
    "        col_ = pd.concat([col_baseline, col_sample], ignore_index=True)\n",
    "\n",
    "        arr = (\n",
    "            col_.groupby([col, \"source\"])\n",
    "            .size()\n",
    "            .to_frame()\n",
    "            .reset_index()\n",
    "            .pivot(index=col, columns=\"source\")\n",
    "            .droplevel(0, axis=1)\n",
    "        )\n",
    "        arr_ = arr.div(arr.sum(axis=0), axis=1)\n",
    "        arr_.fillna(0, inplace=True)\n",
    "        js_distance = jensenshannon(\n",
    "            arr_[\"baseline\"].to_numpy(), arr_[\"sample\"].to_numpy()\n",
    "        )\n",
    "        res.update({col: js_distance})\n",
    "\n",
    "    for col in numerical_columns:\n",
    "        # fit guassian_kde\n",
    "        col_baseline = df1[col]\n",
    "        col_sample = df2[col]\n",
    "        kde_baseline = gaussian_kde(col_baseline)\n",
    "        kde_sample = gaussian_kde(col_sample)\n",
    "\n",
    "        # get range of values\n",
    "        min_ = min(col_baseline.min(), col_sample.min())\n",
    "        max_ = max(col_baseline.max(), col_sample.max())\n",
    "        range_ = np.linspace(start=min_, stop=max_, num=STEPS)\n",
    "\n",
    "        # sample range from KDE\n",
    "        arr_baseline_ = kde_baseline(range_)\n",
    "        arr_sample_ = kde_sample(range_)\n",
    "\n",
    "        arr_baseline = arr_baseline_ / np.sum(arr_baseline_)\n",
    "        arr_sample = arr_sample_ / np.sum(arr_sample_)\n",
    "\n",
    "        # calculate js distance\n",
    "        js_distance = jensenshannon(arr_baseline, arr_sample)\n",
    "\n",
    "        res.update({col: js_distance})\n",
    "\n",
    "    list_output = sorted(res.items(), key=lambda x: x[1], reverse=True)\n",
    "    dict_output = dict(list_output)\n",
    "    return {\n",
    "        \"test_name\": \"Jensen-Shannon\",\n",
    "        \"test_category\": \"data_drift\",\n",
    "        \"test_type\": \"jensen_shannon\",\n",
    "        \"metric\": \"distance\",\n",
    "        \"test_id\": \"data_drift_jensen_shannon_distance\",\n",
    "        \"values\": dict_output\n",
    "    }\n",
    "\n",
    "\n",
    "def get_shap_values(data):\n",
    "    # getting shap values for the test data\n",
    "    shap_values = explainer.shap_values(data)\n",
    "\n",
    "    # re-organizing and sorting the data\n",
    "    shap_values = np.mean(abs(shap_values), axis=0).tolist()\n",
    "    shap_values = dict(zip(data.columns, shap_values))\n",
    "    sorted_shap_values = {\n",
    "        k: v for k, v in sorted(shap_values.items(), key=lambda x: x[1])\n",
    "    }\n",
    "\n",
    "    # show the values\n",
    "    return {\n",
    "        \"test_name\": \"SHAP\",\n",
    "        \"test_category\": \"interpretability\",\n",
    "        \"test_type\": \"shap\",\n",
    "        \"metric\": \"feature_importance\",\n",
    "        \"test_id\": \"interpretability_shap_feature_importance\",\n",
    "        \"values\": sorted_shap_values\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four main sections that are standard to almost any model in MOC:\n",
    "1. Library imports\n",
    "2. `init` function\n",
    "3. `score` function\n",
    "4. `metrics` function\n",
    "\n",
    "**Library** imports are always at the top. We don't need to include all libraries that we used for training and model evaluation. We just need the libraries for processing and scoring.\n",
    "\n",
    "The **`init`** function runs once per deployment, and is used to load and persist into memory any variable that needs to be accessed at scoring time. For example, the init function is where we load the saved model binary. We make the variable global so it can be accessed from the scoring function.\n",
    "\n",
    "The **`score`** function is the function that runs anytime we make a scoring (prediction) request. This is where we put our prediction code. We have to remember to include any steps that were not captured by the pipeline, such as feature engineering or re-encoding.\n",
    "\n",
    "The **`metrics`** functions is where model evaluation is carried out. In our example, this is the place where we replicate the calculations of bias, drift metrics, explainability, and performance metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test our source code to see if we missed anything. We will load input data and scored input data to test both the scoring and metrics functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_sample = pd.read_json('df_baseline.json', lines=True, orient='records')\n",
    "metrics_baseline = pd.read_json('df_baseline_scored.json', lines=True, orient='records')\n",
    "metrics_sample = pd.read_json('df_sample_scored.json', lines=True, orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the **`init`** function can load the trained model binary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No errors from the **`init`** function. Let us now call the **`score`** function on input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = next(action(scoring_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    id  duration_months  credit_amount  installment_rate  \\\n",
       "0  687               36           2862                 4   \n",
       "1  500               24           3123                 4   \n",
       "2  332               60           7408                 4   \n",
       "3  979               15           1264                 2   \n",
       "4  817                6           1554                 1   \n",
       "\n",
       "   present_residence_since  age_years  number_existing_credits  \\\n",
       "0                        3         30                        1   \n",
       "1                        1         27                        1   \n",
       "2                        2         24                        1   \n",
       "3                        2         25                        1   \n",
       "4                        2         24                        2   \n",
       "\n",
       "  checking_status credit_history purpose  ... installment_plans housing   job  \\\n",
       "0             A12            A33     A40  ...              A143    A153  A173   \n",
       "1             A11            A32     A40  ...              A143    A152  A173   \n",
       "2             A12            A32     A40  ...              A143    A152  A174   \n",
       "3             A12            A31     A40  ...              A143    A151  A173   \n",
       "4             A14            A34     A43  ...              A143    A151  A173   \n",
       "\n",
       "  number_people_liable telephone foreign_worker  gender  label_value score  \\\n",
       "0                    1      A191           A201    male            0     1   \n",
       "1                    1      A191           A201  female            1     1   \n",
       "2                    1      A191           A201  female            1     1   \n",
       "3                    1      A191           A201    male            1     1   \n",
       "4                    1      A192           A201  female            0     0   \n",
       "\n",
       "  predicted_probs  \n",
       "0        0.785607  \n",
       "1        0.848943  \n",
       "2        0.940270  \n",
       "3        0.684183  \n",
       "4        0.109081  \n",
       "\n",
       "[5 rows x 24 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>duration_months</th>\n      <th>credit_amount</th>\n      <th>installment_rate</th>\n      <th>present_residence_since</th>\n      <th>age_years</th>\n      <th>number_existing_credits</th>\n      <th>checking_status</th>\n      <th>credit_history</th>\n      <th>purpose</th>\n      <th>...</th>\n      <th>installment_plans</th>\n      <th>housing</th>\n      <th>job</th>\n      <th>number_people_liable</th>\n      <th>telephone</th>\n      <th>foreign_worker</th>\n      <th>gender</th>\n      <th>label_value</th>\n      <th>score</th>\n      <th>predicted_probs</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>687</td>\n      <td>36</td>\n      <td>2862</td>\n      <td>4</td>\n      <td>3</td>\n      <td>30</td>\n      <td>1</td>\n      <td>A12</td>\n      <td>A33</td>\n      <td>A40</td>\n      <td>...</td>\n      <td>A143</td>\n      <td>A153</td>\n      <td>A173</td>\n      <td>1</td>\n      <td>A191</td>\n      <td>A201</td>\n      <td>male</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.785607</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>500</td>\n      <td>24</td>\n      <td>3123</td>\n      <td>4</td>\n      <td>1</td>\n      <td>27</td>\n      <td>1</td>\n      <td>A11</td>\n      <td>A32</td>\n      <td>A40</td>\n      <td>...</td>\n      <td>A143</td>\n      <td>A152</td>\n      <td>A173</td>\n      <td>1</td>\n      <td>A191</td>\n      <td>A201</td>\n      <td>female</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.848943</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>332</td>\n      <td>60</td>\n      <td>7408</td>\n      <td>4</td>\n      <td>2</td>\n      <td>24</td>\n      <td>1</td>\n      <td>A12</td>\n      <td>A32</td>\n      <td>A40</td>\n      <td>...</td>\n      <td>A143</td>\n      <td>A152</td>\n      <td>A174</td>\n      <td>1</td>\n      <td>A191</td>\n      <td>A201</td>\n      <td>female</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.940270</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>979</td>\n      <td>15</td>\n      <td>1264</td>\n      <td>2</td>\n      <td>2</td>\n      <td>25</td>\n      <td>1</td>\n      <td>A12</td>\n      <td>A31</td>\n      <td>A40</td>\n      <td>...</td>\n      <td>A143</td>\n      <td>A151</td>\n      <td>A173</td>\n      <td>1</td>\n      <td>A191</td>\n      <td>A201</td>\n      <td>male</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.684183</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>817</td>\n      <td>6</td>\n      <td>1554</td>\n      <td>1</td>\n      <td>2</td>\n      <td>24</td>\n      <td>2</td>\n      <td>A14</td>\n      <td>A34</td>\n      <td>A43</td>\n      <td>...</td>\n      <td>A143</td>\n      <td>A151</td>\n      <td>A173</td>\n      <td>1</td>\n      <td>A192</td>\n      <td>A201</td>\n      <td>female</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.109081</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 24 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "pd.DataFrame(scores).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have scores! Last but not least, let's call the **`metrics`** function on scored data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "get_disparity_predefined_group()\n",
      "Estimated covariance matrix does not have full rank. This indicates a bad choice of the input t and the test might not be consistent.\n",
      "divide by zero encountered in true_divide\n",
      "invalid value encountered in cos\n",
      "invalid value encountered in sin\n",
      "invalid value encountered in cos\n",
      "invalid value encountered in sin\n"
     ]
    }
   ],
   "source": [
    "metrics_output = next(metrics(metrics_baseline, metrics_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'performance': [{'test_name': 'Classification Metrics',\n",
       "   'test_category': 'performance',\n",
       "   'test_type': 'classification_metrics',\n",
       "   'test_id': 'performance_classification_metrics',\n",
       "   'values': {'f1_score': 0.54421768707483,\n",
       "    'auc': 0.7408936376881982,\n",
       "    'confusion_matrix': [{'Default': 93, 'Pay Off': 49},\n",
       "     {'Default': 18, 'Pay Off': 40}]}}],\n",
       " 'confusion_matrix': [{'Default': 93, 'Pay Off': 49},\n",
       "  {'Default': 18, 'Pay Off': 40}],\n",
       " 'roc': [{'fpr': 0.0, 'tpr': 0.0},\n",
       "  {'fpr': 0.0, 'tpr': 0.017241379310344827},\n",
       "  {'fpr': 0.007042253521126761, 'tpr': 0.017241379310344827},\n",
       "  {'fpr': 0.007042253521126761, 'tpr': 0.10344827586206896},\n",
       "  {'fpr': 0.014084507042253521, 'tpr': 0.10344827586206896},\n",
       "  {'fpr': 0.014084507042253521, 'tpr': 0.1724137931034483},\n",
       "  {'fpr': 0.035211267605633804, 'tpr': 0.1724137931034483},\n",
       "  {'fpr': 0.035211267605633804, 'tpr': 0.22413793103448276},\n",
       "  {'fpr': 0.04225352112676056, 'tpr': 0.22413793103448276},\n",
       "  {'fpr': 0.04225352112676056, 'tpr': 0.25862068965517243},\n",
       "  {'fpr': 0.06338028169014084, 'tpr': 0.25862068965517243},\n",
       "  {'fpr': 0.06338028169014084, 'tpr': 0.3275862068965517},\n",
       "  {'fpr': 0.07746478873239436, 'tpr': 0.3275862068965517},\n",
       "  {'fpr': 0.07746478873239436, 'tpr': 0.3448275862068966},\n",
       "  {'fpr': 0.08450704225352113, 'tpr': 0.3448275862068966},\n",
       "  {'fpr': 0.08450704225352113, 'tpr': 0.39655172413793105},\n",
       "  {'fpr': 0.11267605633802817, 'tpr': 0.39655172413793105},\n",
       "  {'fpr': 0.11267605633802817, 'tpr': 0.41379310344827586},\n",
       "  {'fpr': 0.13380281690140844, 'tpr': 0.41379310344827586},\n",
       "  {'fpr': 0.13380281690140844, 'tpr': 0.43103448275862066},\n",
       "  {'fpr': 0.14788732394366197, 'tpr': 0.43103448275862066},\n",
       "  {'fpr': 0.14788732394366197, 'tpr': 0.4827586206896552},\n",
       "  {'fpr': 0.15492957746478872, 'tpr': 0.4827586206896552},\n",
       "  {'fpr': 0.15492957746478872, 'tpr': 0.5},\n",
       "  {'fpr': 0.1619718309859155, 'tpr': 0.5},\n",
       "  {'fpr': 0.1619718309859155, 'tpr': 0.5172413793103449},\n",
       "  {'fpr': 0.176056338028169, 'tpr': 0.5172413793103449},\n",
       "  {'fpr': 0.176056338028169, 'tpr': 0.5517241379310345},\n",
       "  {'fpr': 0.22535211267605634, 'tpr': 0.5517241379310345},\n",
       "  {'fpr': 0.22535211267605634, 'tpr': 0.5689655172413793},\n",
       "  {'fpr': 0.2535211267605634, 'tpr': 0.5689655172413793},\n",
       "  {'fpr': 0.2535211267605634, 'tpr': 0.5862068965517241},\n",
       "  {'fpr': 0.28169014084507044, 'tpr': 0.5862068965517241},\n",
       "  {'fpr': 0.28169014084507044, 'tpr': 0.603448275862069},\n",
       "  {'fpr': 0.2887323943661972, 'tpr': 0.603448275862069},\n",
       "  {'fpr': 0.2887323943661972, 'tpr': 0.6206896551724138},\n",
       "  {'fpr': 0.29577464788732394, 'tpr': 0.6206896551724138},\n",
       "  {'fpr': 0.29577464788732394, 'tpr': 0.6379310344827587},\n",
       "  {'fpr': 0.3028169014084507, 'tpr': 0.6379310344827587},\n",
       "  {'fpr': 0.3028169014084507, 'tpr': 0.6551724137931034},\n",
       "  {'fpr': 0.31690140845070425, 'tpr': 0.6551724137931034},\n",
       "  {'fpr': 0.31690140845070425, 'tpr': 0.6724137931034483},\n",
       "  {'fpr': 0.3380281690140845, 'tpr': 0.6724137931034483},\n",
       "  {'fpr': 0.3380281690140845, 'tpr': 0.6896551724137931},\n",
       "  {'fpr': 0.352112676056338, 'tpr': 0.6896551724137931},\n",
       "  {'fpr': 0.352112676056338, 'tpr': 0.7068965517241379},\n",
       "  {'fpr': 0.3873239436619718, 'tpr': 0.7068965517241379},\n",
       "  {'fpr': 0.3873239436619718, 'tpr': 0.7413793103448276},\n",
       "  {'fpr': 0.4014084507042254, 'tpr': 0.7413793103448276},\n",
       "  {'fpr': 0.4014084507042254, 'tpr': 0.7586206896551724},\n",
       "  {'fpr': 0.4507042253521127, 'tpr': 0.7586206896551724},\n",
       "  {'fpr': 0.4507042253521127, 'tpr': 0.7758620689655172},\n",
       "  {'fpr': 0.45774647887323944, 'tpr': 0.7758620689655172},\n",
       "  {'fpr': 0.45774647887323944, 'tpr': 0.7931034482758621},\n",
       "  {'fpr': 0.47183098591549294, 'tpr': 0.7931034482758621},\n",
       "  {'fpr': 0.47183098591549294, 'tpr': 0.8103448275862069},\n",
       "  {'fpr': 0.5845070422535211, 'tpr': 0.8103448275862069},\n",
       "  {'fpr': 0.5845070422535211, 'tpr': 0.8275862068965517},\n",
       "  {'fpr': 0.6126760563380281, 'tpr': 0.8275862068965517},\n",
       "  {'fpr': 0.6126760563380281, 'tpr': 0.8620689655172413},\n",
       "  {'fpr': 0.6267605633802817, 'tpr': 0.8620689655172413},\n",
       "  {'fpr': 0.6267605633802817, 'tpr': 0.896551724137931},\n",
       "  {'fpr': 0.647887323943662, 'tpr': 0.896551724137931},\n",
       "  {'fpr': 0.647887323943662, 'tpr': 0.9310344827586207},\n",
       "  {'fpr': 0.7112676056338029, 'tpr': 0.9310344827586207},\n",
       "  {'fpr': 0.7112676056338029, 'tpr': 0.9482758620689655},\n",
       "  {'fpr': 0.795774647887324, 'tpr': 0.9482758620689655},\n",
       "  {'fpr': 0.795774647887324, 'tpr': 0.9655172413793104},\n",
       "  {'fpr': 0.8450704225352113, 'tpr': 0.9655172413793104},\n",
       "  {'fpr': 0.8450704225352113, 'tpr': 0.9827586206896551},\n",
       "  {'fpr': 0.8802816901408451, 'tpr': 0.9827586206896551},\n",
       "  {'fpr': 0.8802816901408451, 'tpr': 1.0},\n",
       "  {'fpr': 1.0, 'tpr': 1.0}],\n",
       " 'bias': {'test_name': 'Aequitas Bias',\n",
       "  'test_category': 'bias',\n",
       "  'test_type': 'bias',\n",
       "  'protected_class': 'race',\n",
       "  'test_id': 'bias_bias_gender',\n",
       "  'reference_group': 'male',\n",
       "  'thresholds': {'min': 0.8, 'max': 1.25},\n",
       "  'values': [[{'attribute_name': 'gender',\n",
       "     'attribute_value': 'female',\n",
       "     'ppr_disparity': 0.4590163934426229,\n",
       "     'pprev_disparity': 0.8160291438979964,\n",
       "     'precision_disparity': 1.4523809523809523,\n",
       "     'fdr_disparity': 0.7065637065637065,\n",
       "     'for_disparity': 1.522727272727273,\n",
       "     'fpr_disparity': 0.6555491661874641,\n",
       "     'fnr_disparity': 1.32,\n",
       "     'tpr_disparity': 0.88,\n",
       "     'tnr_disparity': 1.2197358767424797,\n",
       "     'npv_disparity': 0.9188871473354232},\n",
       "    {'attribute_name': 'gender',\n",
       "     'attribute_value': 'male',\n",
       "     'ppr_disparity': 1.0,\n",
       "     'pprev_disparity': 1.0,\n",
       "     'precision_disparity': 1.0,\n",
       "     'fdr_disparity': 1.0,\n",
       "     'for_disparity': 1.0,\n",
       "     'fpr_disparity': 1.0,\n",
       "     'fnr_disparity': 1.0,\n",
       "     'tpr_disparity': 1.0,\n",
       "     'tnr_disparity': 1.0,\n",
       "     'npv_disparity': 1.0}]]},\n",
       " 'data_drift': [{'test_name': 'Epps-Singleton',\n",
       "   'test_category': 'data_drift',\n",
       "   'test_type': 'epps_singleton',\n",
       "   'metric': 'p_value',\n",
       "   'test_id': 'data_drift_epps_singleton_p_value',\n",
       "   'values': {'duration_months': 0.7865371028121152,\n",
       "    'credit_amount': 0.422690445604187,\n",
       "    'installment_rate': 0.42361856358384253,\n",
       "    'present_residence_since': 0.3441580301969012,\n",
       "    'age_years': 0.017852555876398106,\n",
       "    'number_existing_credits': 0.6695508307553562,\n",
       "    'number_people_liable': None}},\n",
       "  {'test_name': 'Kolmogorov-Smirnov',\n",
       "   'test_category': 'data_drift',\n",
       "   'test_type': 'kolmogorov_smirnov',\n",
       "   'metric': 'p_value',\n",
       "   'test_id': 'data_drift_kolmogorov_smirnov_p_value',\n",
       "   'values': {'duration_months': 0.47212809343243023,\n",
       "    'credit_amount': 0.5733039905352183,\n",
       "    'installment_rate': 0.783252147486204,\n",
       "    'present_residence_since': 0.8076312171824567,\n",
       "    'age_years': 0.2495370487521852,\n",
       "    'number_existing_credits': 0.9999992134210486,\n",
       "    'number_people_liable': 0.999999990315373}},\n",
       "  {'test_name': 'Jensen-Shannon',\n",
       "   'test_category': 'data_drift',\n",
       "   'test_type': 'jensen_shannon',\n",
       "   'metric': 'distance',\n",
       "   'test_id': 'data_drift_jensen_shannon_distance',\n",
       "   'values': {'number_existing_credits': 0.1661573547955825,\n",
       "    'number_people_liable': 0.15643137378956762,\n",
       "    'present_residence_since': 0.09589797018577487,\n",
       "    'installment_rate': 0.0923431071984353,\n",
       "    'purpose': 0.08901857345091589,\n",
       "    'credit_amount': 0.06579792762987813,\n",
       "    'age_years': 0.062297942889786954,\n",
       "    'present_employment_since': 0.06092273471356515,\n",
       "    'duration_months': 0.05571648405502104,\n",
       "    'savings_account': 0.04709760284899042,\n",
       "    'gender': 0.04706585708757519,\n",
       "    'credit_history': 0.03574135542427669,\n",
       "    'property': 0.03477985804578699,\n",
       "    'telephone': 0.026242637204975147,\n",
       "    'job': 0.024366893631309054,\n",
       "    'foreign_worker': 0.01806487817678789,\n",
       "    'checking_status': 0.01600519274897279,\n",
       "    'installment_plans': 0.015781433298469323,\n",
       "    'housing': 0.010258094196074643,\n",
       "    'debtors_guarantors': 0.004689237456452252}}],\n",
       " 'concept_drift': [{'test_name': 'Epps-Singleton',\n",
       "   'test_category': 'data_drift',\n",
       "   'test_type': 'epps_singleton',\n",
       "   'metric': 'p_value',\n",
       "   'test_id': 'data_drift_epps_singleton_p_value',\n",
       "   'values': {'score': 0.5663889078033799}},\n",
       "  {'test_name': 'Kolmogorov-Smirnov',\n",
       "   'test_category': 'data_drift',\n",
       "   'test_type': 'kolmogorov_smirnov',\n",
       "   'metric': 'p_value',\n",
       "   'test_id': 'data_drift_kolmogorov_smirnov_p_value',\n",
       "   'values': {'score': 0.9999958279505915}},\n",
       "  {'test_name': 'Jensen-Shannon',\n",
       "   'test_category': 'data_drift',\n",
       "   'test_type': 'jensen_shannon',\n",
       "   'metric': 'distance',\n",
       "   'test_id': 'data_drift_jensen_shannon_distance',\n",
       "   'values': {'score': 0.13785752198843274}}],\n",
       " 'explainability': [{'test_name': 'SHAP',\n",
       "   'test_category': 'interpretability',\n",
       "   'test_type': 'shap',\n",
       "   'metric': 'feature_importance',\n",
       "   'test_id': 'interpretability_shap_feature_importance',\n",
       "   'values': {'job_A171': 3.1328221263664673e-05,\n",
       "    'purpose_A410': 0.00020948758335020458,\n",
       "    'purpose_A44': 0.0003090591737098107,\n",
       "    'purpose_A48': 0.0008766581825881572,\n",
       "    'debtors_guarantors_A101': 0.0009150508136599006,\n",
       "    'housing_A153': 0.0016935646612710384,\n",
       "    'present_employment_since_A71': 0.0017089631277168033,\n",
       "    'savings_account_A63': 0.0020129763606446262,\n",
       "    'purpose_A45': 0.0020844768187666747,\n",
       "    'foreign_worker_A201': 0.003766964306474573,\n",
       "    'purpose_A42': 0.0039455273948171485,\n",
       "    'checking_status_A13': 0.006138224622855559,\n",
       "    'installment_plans_A142': 0.006372036544310882,\n",
       "    'job_A172': 0.006583504312811813,\n",
       "    'foreign_worker_A202': 0.006782362236824462,\n",
       "    'number_existing_credits': 0.00750011359963024,\n",
       "    'purpose_A49': 0.007500582081896385,\n",
       "    'savings_account_A64': 0.00801772942021382,\n",
       "    'credit_history_A33': 0.00805456018203451,\n",
       "    'debtors_guarantors_A102': 0.009021527006583886,\n",
       "    'credit_history_A30': 0.011292419424637157,\n",
       "    'savings_account_A62': 0.01194298101274311,\n",
       "    'property_A123': 0.012660923236592878,\n",
       "    'debtors_guarantors_A103': 0.01381481850812078,\n",
       "    'credit_history_A31': 0.014032983160095276,\n",
       "    'number_people_liable_2': 0.015317375151231539,\n",
       "    'job_A174': 0.01674088696555357,\n",
       "    'credit_history_A32': 0.017734229446707925,\n",
       "    'purpose_A46': 0.019873506687696822,\n",
       "    'property_A122': 0.01995330431488622,\n",
       "    'present_employment_since_A73': 0.02237516182532416,\n",
       "    'installment_plans_A141': 0.027711132342581327,\n",
       "    'number_people_liable_1': 0.029942891113355786,\n",
       "    'present_employment_since_A75': 0.03893752534980459,\n",
       "    'property_A124': 0.039322747899443185,\n",
       "    'housing_A151': 0.043992108858815444,\n",
       "    'present_residence_since': 0.04729293278716826,\n",
       "    'telephone_A191': 0.05240518751403526,\n",
       "    'purpose_A41': 0.05539897377916361,\n",
       "    'job_A173': 0.06528054604680217,\n",
       "    'present_employment_since_A74': 0.06728753336749936,\n",
       "    'checking_status_A12': 0.06828473759183341,\n",
       "    'present_employment_since_A72': 0.07087086711525754,\n",
       "    'installment_plans_A143': 0.07795073352914118,\n",
       "    'telephone_A192': 0.07971655475877716,\n",
       "    'housing_A152': 0.08128517023081643,\n",
       "    'savings_account_A65': 0.09052881652812012,\n",
       "    'purpose_A43': 0.09734556985402298,\n",
       "    'property_A121': 0.11331949356374771,\n",
       "    'purpose_A40': 0.11944105896776773,\n",
       "    'credit_amount': 0.13377257721960387,\n",
       "    'savings_account_A61': 0.13647436684151096,\n",
       "    'age_years': 0.148075573075796,\n",
       "    'credit_history_A34': 0.18972961710340905,\n",
       "    'checking_status_A11': 0.1940913502412228,\n",
       "    'installment_rate': 0.1942918160734251,\n",
       "    'duration_months': 0.294281718102304,\n",
       "    'checking_status_A14': 0.32966646260987026}}]}"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "metrics_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Done and done!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}